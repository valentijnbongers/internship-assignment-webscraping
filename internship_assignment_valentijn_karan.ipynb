{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valentijnbongers/internship-assignment-webscraping/blob/main/internship_assignment_valentijn_karan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_D6lbc_bOYv"
      },
      "source": [
        "# **Introduction To Web Scraping**\n",
        "\n",
        "# Structure\n",
        "\n",
        "\n",
        "1.   Ethical considerations\n",
        "2.  Legal aspects\n",
        "3.   Techical approachs\n",
        "\n",
        "  *   `BeatifulSoup`\n",
        "  * `requests`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Link to [Github](https://github.com/valentijnbongers/internship-assignment-webscraping)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te3g0ZFSfK12"
      },
      "source": [
        "#### What is web scraping?\n",
        "\n",
        "Web scraping is a technique used to automatically extract information from websites.\n",
        "\n",
        "\n",
        "### **Ethical Considerations:**\n",
        "\n",
        "1. Website Strain: Rate of requests should be limited. Too many requests may violate the website's terms of service and put undue load on the web servers.\n",
        "\n",
        "2. Privacy issues: Scraping of personal information such as emails or phone numbers is not advised unless explicity allowed.\n",
        "\n",
        "3. Data usage: Only scrape data you need. Don't sell os misuse scraped data.\n",
        "\n",
        "### **Legal Aspects:**\n",
        "\n",
        "1. Respecting Robots.txt: Websites can have robots.txt file that tells bots (including web scrapers) which parts of the site they are not allowed to access. Following robots.txt is considered good practice.\n",
        "\n",
        "2. Public Vs. Private Data: Generally, scarping publicily available data is okay, as long as you follow the website's terms of service.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PBo-YPMq4UO"
      },
      "source": [
        "\n",
        "## HTML page structure\n",
        "\n",
        "**Hypertext Markup Language (HTML)** is the standard markup language for documents designed to be displayed in a web browser. HTML describes the structure of a web page and it can be used with **Cascading Style Sheets (CSS)** and a scripting language such as **JavaScript** to create interactive websites. HTML consists of a series of elements that \"tell\" to the browser how to display the content. Lastly, elements are represented by **tags**.\n",
        "\n",
        "Here are some tags:\n",
        "* `<!DOCTYPE html>` declaration defines this document to be HTML5.  \n",
        "* `<html>` element is the root element of an HTML page.  \n",
        "* `<div>` tag defines a division or a section in an HTML document. It's usually a container for other elements.\n",
        "* `<head>` element contains meta information about the document.  \n",
        "* `<title>` element specifies a title for the document.  \n",
        "* `<body>` element contains the visible page content.  \n",
        "* `<h1>` element defines a large heading.  \n",
        "* `<p>` element defines a paragraph.  \n",
        "* `<a>` element defines a hyperlink."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0UeqSKfrKWW"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Web Scraping with `requests` and `BeautifulSoup`\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#Imports\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### What is `BeautifulSoup`?\n",
        "\n",
        "It is a Python library for pulling data out of HTML and XML files. It provides methods to navigate the document's tree structure that we discussed before and scrape its content.\n",
        "\n",
        "### What is `requests`?\n",
        "\n",
        "It is a Python library that allows you to send HTTP requests to websites and retrieve their responses. It simplifies the process of fetching web content compared to using the built-in `urllib` library in Python.\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:1400/1*atk74YuoKuEn_b-Qs8nOgg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaPTyBHH_JtD"
      },
      "source": [
        "#### Web Parsing Tasks\n",
        "* Find the _Sign In_ button\n",
        "* Find the Shopee logo.\n",
        "* Locate one of products in the main section of the page.\n",
        "* What is the _heading_ size of the titles in the main section of the page?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcRB7rY0LnM-"
      },
      "source": [
        "# **Website Structure**\n",
        "\n",
        "# Structure\n",
        "* Lazada & Shopee\n",
        "  * Textbox\n",
        "  * Result Page\n",
        "  * Visuals of products or services"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwPHSVnnRi1s"
      },
      "source": [
        "### Overall Structure\n",
        "\n",
        "\n",
        "Both websites have a very similar structure on the main page, including a search bar at the top, followed by some promotions and categories of products to chooose from. Smaller details were also similar for both websites. Like a shopping cart in the top right and a logo in the top left.\n",
        "\n",
        "## Textbox:\n",
        "\n",
        "<img src='https://blog.leapecommerce.com/content/images/2022/06/Shopee-Auto-Complete-1.png' width = 500>\n",
        "\n",
        "There is always a textbox at the top of the homepage, which allows website visitors to type in their “search” directly to find what they are looking for.   \n",
        "\n",
        "## Result page:\n",
        "\n",
        "<img src='https://thelead.io/wp-content/uploads/2018/11/Screen-Shot-2018-11-06-at-1.19.29-AM.png' width=500>\n",
        "\n",
        "After you key in what you are looking for, the website will redirect you to a filtered display of what you are looking for according to the keywords you have entered earlier in the textbox.  \n",
        "\n",
        "## Visuals of products or services:\n",
        "\n",
        "<img src='https://beebot-sg-knowledgecloud.oss-ap-southeast-1.aliyuncs.com/kc/kc-media/kc-oss-1690256248802-image.png' width=500>\n",
        "\n",
        "Another key similarity you will see on e-commerce sites, such as Shopee and Lazada, is high-resolution product images from multiple angles, lifestyle photos, infographics, videos, and AR features, all organized in a clean, branded layout to enhance the shopping experience.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0v0-7WPXqqX"
      },
      "source": [
        "## Differences\n",
        "\n",
        "While both are similar in a lot of ways, there are some noticible differences that allow you to seperate these 2 e-commerce platforms.\n",
        "\n",
        "1. Homepage Layout:\n",
        "    * Lazada: Features a more traditional e-commerce layout with a prominent search bar, rotating banners, and categorized product listings. It often highlights major sales and promotions at the top.\n",
        "    * Shopee: Utilizes a mobile-first design, with a focus on interactive features like Shopee Live, flash sales, and games. The homepage often has more vibrant and dynamic elements.\n",
        "2. Product Pages:\n",
        "    * Lazada: Provides detailed product descriptions, specifications, and user reviews. It also includes a comparison feature and related product suggestions.\n",
        "    * Shopee: Focuses on user-generated content, with a heavy emphasis on customer reviews and ratings. It often includes interactive elements like Q&A sections and short video reviews.\n",
        "3. Navigation:\n",
        "    * Lazada: Offers a more structured navigation with clear categories and subcategories. It often includes filters and sorting options for easier product search.\n",
        "    * Shopee: Emphasizes a simpler navigation structure that relies heavily on search and discovery through personalized recommendations and trending products.\n",
        "4. HTML elements:\n",
        "    * The two websites are also different in the code supporting it. For example, they use different tags and attribute names. This will require some changes in the webscraping code for each website."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cy-V4sO8ekr"
      },
      "source": [
        "# **HTML Elements:**\n",
        "\n",
        "## Product names, descriptions, Prices\n",
        "\n",
        "### HTML elements are made of 3 components;\n",
        "* an opening tag,\n",
        "* the content,\n",
        "* and a closing tag.\n",
        "\n",
        "They can be found by 'inspecting' a web page\n",
        "\n",
        "\n",
        "<img src='https://assets.digitalocean.com/django_gunicorn_nginx_2004/articles/new_learners/html-element-diagram.png' width=250>\n",
        "\n",
        "The opening tag contains the tag name, the attribute name, and the attribute value.\n",
        "The content contains what will be written onto the web page.\n",
        "\n",
        "Example from a title on Lazada:\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1rlBSoCaT0u3-31K1jAnqOUWFD8em3Cv8' height=40 width=500>\n",
        "\n",
        "\n",
        "On Lazada, all product titles have a tag of `'h1'` and an attribute name of `'pdp-mod-product-badge-title'`. All prices have a tag of `'span'` and an attribute name of `'notranslate pdp-price pdp-price_type_normal pdp-price_color_orange pdp-price_size_xl'`. All descriptions have a tag of 'article and a atterbute name of 'lzd-article'.\n",
        "\n",
        "On shopee, titles have tags `'div'` and attribute name `'WBVL_7'`. Prices had tags `'div'` and attribute name `'G27FPf'`. Descriptions have tags 'div' and name `'e8lZp3'`. These tags and attribute names can be used to filter out, and print only wanted data. This also allows data to be sorted into titles, prices, and descriptions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "898AwOVKZCmA"
      },
      "source": [
        "# Scraping data from a single product page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9gg5UMB_ZFfr",
        "outputId": "ea706370-16b8-4c2a-ef3d-c94b30e8f7b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "suK39YHNucU7"
      },
      "outputs": [],
      "source": [
        "from typing import Text\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbY8Me_yheIb"
      },
      "source": [
        "# Scraping from a whole search page"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CwrZSHoieH_"
      },
      "source": [
        "[Test Page 1 ](https://www.ebay.com.my/sch/Car-Truck-Parts/6030/i.html?_from=R40&LH_BIN=1&_nkw=Mercedes+Benz+-Kaidon&rt=nc&_trkparms=parentrq%3A4d7233ae1900a72e7b702ecafffdd672%7Cpageci%3A508e36ed-32a3-11ef-bac3-6adee66f9ad8%7Cc%3A1%7Ciid%3A1%7Cli%3A8874)\n",
        "\n",
        "[Test Page 2 ](https://www.ebay.com.my/sch/Car-Truck-Parts/6030/i.html?_from=R40&LH_BIN=1&_nkw=Mercedes+Benz+-Kaidon&rt=nc&_trkparms=parentrq%3A4d7233ae1900a72e7b702ecafffdd672%7Cpageci%3A508e36ed-32a3-11ef-bac3-6adee66f9ad8%7Cc%3A1%7Ciid%3A1%7Cli%3A8874&LH_PrefLoc=2&imm=1&_pgn=2)\n",
        "\n",
        "\n",
        "[Test Page 3](https://www.ebay.com.my/sch/Car-Truck-Parts/6030/i.html?_from=R40&LH_BIN=1&_nkw=Mercedes+Benz+-Kaidon&rt=nc&_trkparms=parentrq%3A4d7233ae1900a72e7b702ecafffdd672%7Cpageci%3A508e36ed-32a3-11ef-bac3-6adee66f9ad8%7Cc%3A1%7Ciid%3A1%7Cli%3A8874&LH_PrefLoc=2&imm=1&_pgn=3)\n",
        "\n",
        "\n",
        "....\n",
        "\n",
        "\n",
        "\n",
        "*etc.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions for collecting product information\n",
        "\n",
        "#Product name\n",
        "def get_name(soup):\n",
        "  content = soup.find('div', id = 'mainContent')\n",
        "  product_name = content.find('span', class_ = 'ux-textspans ux-textspans--BOLD').text\n",
        "  return product_name\n",
        "\n",
        "#Product price\n",
        "def get_price(soup):\n",
        "  product_price = soup.find('div', class_ = 'x-price-primary').text\n",
        "  return product_price\n",
        "\n",
        "#Product condition\n",
        "def get_condition(soup):\n",
        "  product_condition = soup.find('span', class_ = 'ux-icon-text__text').text\n",
        "  return product_condition\n",
        "\n",
        "#Function for scraping the whole page\n",
        "def scrape_product_page(page_url):\n",
        "  response = requests.get(page_url)\n",
        "  soup = BeautifulSoup(response.content,'html.parser')\n",
        "  name = get_name(soup)\n",
        "  price = get_price(soup)\n",
        "  condition = get_condition(soup)\n",
        "  if condition != '--not specified':\n",
        "    condition = condition[:int((len(condition)/2))]\n",
        "  arr = [name, price, condition]\n",
        "  return arr"
      ],
      "metadata": {
        "id": "PknYQ_5s4Fk5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JcelvTVzAuy7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Finds all products with selected key word\n",
        "\n",
        "def scrape_search_page(url,num_pages):\n",
        "  items_array = []\n",
        "  for k in range(1, num_pages + 1):\n",
        "\n",
        "    paged_url = url + \"&_pgn=\" + str(k)\n",
        "    response2 = requests.get(paged_url)\n",
        "    soup2 = BeautifulSoup(response2.content, 'html.parser')\n",
        "    all_items = soup2.find('div', class_ = 'srp-river-results clearfix')\n",
        "    items = all_items.find_all('li', class_ = 's-item s-item__pl-on-bottom')\n",
        "\n",
        "  #Displays information\n",
        "\n",
        "    for item in items:\n",
        "      try:\n",
        "        #finds element that contains link to product page\n",
        "        link_element = item.find('a', href = True)\n",
        "\n",
        "        #finds link to product page\n",
        "        href = link_element.get('href')\n",
        "        array_data = scrape_product_page(href)\n",
        "        items_array.append(array_data)\n",
        "      except AttributeError:\n",
        "        pass\n",
        "      else:\n",
        "        pass\n",
        "      finally:\n",
        "        pass\n",
        "    return items_array"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = scrape_search_page('https://www.ebay.com.my/sch/i.html?_from=R40&_nkw=iPhone&LH_PrefLoc=2&imm=1', 100)\n",
        "print(len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzDVXsVZrT9g",
        "outputId": "6c6ca273-fe21-4371-8ca1-8b98fb13c3c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29G5qZPUj9H0"
      },
      "source": [
        "## Handling pagination:\n",
        "[Test Page 1:](https://www.ebay.com.my/sch/Car-Truck-Parts/6030/i.html?_from=R40&LH_BIN=1&_nkw=Mercedes+Benz+-Kaidon&rt=nc&_trkparms=parentrq%3A4d7233ae1900a72e7b702ecafffdd672%7Cpageci%3A508e36ed-32a3-11ef-bac3-6adee66f9ad8%7Cc%3A1%7Ciid%3A1%7Cli%3A8874) url + &LH_PrefLoc=2&imm=1&_pgn=1\n",
        "\n",
        "\n",
        "[Test Page 2:](https://www.ebay.com.my/sch/Car-Truck-Parts/6030/i.html?_from=R40&LH_BIN=1&_nkw=Mercedes+Benz+-Kaidon&rt=nc&_trkparms=parentrq%3A4d7233ae1900a72e7b702ecafffdd672%7Cpageci%3A508e36ed-32a3-11ef-bac3-6adee66f9ad8%7Cc%3A1%7Ciid%3A1%7Cli%3A8874&LH_PrefLoc=2&imm=1&_pgn=2) url + &LH_PrefLoc=2&imm=1&_pgn=2\n",
        "\n",
        "[Test Page 3:](https://www.ebay.com.my/sch/Car-Truck-Parts/6030/i.html?_from=R40&LH_BIN=1&_nkw=Mercedes+Benz+-Kaidon&rt=nc&_trkparms=parentrq%3A4d7233ae1900a72e7b702ecafffdd672%7Cpageci%3A508e36ed-32a3-11ef-bac3-6adee66f9ad8%7Cc%3A1%7Ciid%3A1%7Cli%3A8874&LH_PrefLoc=2&imm=1&_pgn=3) url + &LH_PrefLoc=2&imm=1&_pgn=3\n",
        "\n",
        "....\n",
        "\n",
        "\n",
        "*etc.*\n",
        "\n",
        "-------\n",
        "\n",
        "However, `'&LH_PrefLoc=2&imm=1'` is unnecesary and doesnt affect the page it is linked to so subsequent pages can be accessed by appending `'&_pgn=k'` onto the first page url, where k is the page number. (first page can also be represented as `url + '&_pgn=1'`)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math"
      ],
      "metadata": {
        "id": "xufBiht9EOnQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to extract all integers from a string and concatenate them\n",
        "def extract_int(string):\n",
        "  integers = re.findall(r'\\d+', string)\n",
        "  ints = ''.join(integers)\n",
        "  return ints"
      ],
      "metadata": {
        "id": "BRR-T1diGPnw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string1 = '20,000'\n",
        "print(extract_int(string1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8eOpFnAn8x2",
        "outputId": "6b941509-20b1-4293-9c85-a2716836273b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_num_pages(url):\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "  num_items_phrase = soup.find('div', class_ = 'srp-controls__control srp-controls__count')\n",
        "  num_items = num_items_phrase.find('span', class_ = 'BOLD').text\n",
        "  num_items = int(extract_int(num_items))\n",
        "  num_of_pages = math.ceil(num_items/60)\n",
        "  return num_of_pages"
      ],
      "metadata": {
        "id": "PKWB-7erAho3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_url = 'https://www.ebay.com.my/sch/i.html?_from=R40&_trksid=m570.l1313&_nkw=iPhone&_sacat=0'\n",
        "find_num_pages(test_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afocZwJjBhBw",
        "outputId": "a8f1eca8-1dd7-4d40-dbb2-ee270324cafc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "367"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QroJnEp3mXbG"
      },
      "source": [
        "## Transferring data to Excel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hcGd4VJb9SQD",
        "outputId": "84eddfbc-aa38-41f8-f7af-d8507f51971a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xlsxwriter"
      ],
      "metadata": {
        "id": "rc_Mjfx6UBSc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CCETZ5ynmVLQ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def export_to_excel(items_array, sheet_number, file_name):\n",
        "  # Create a DataFrame from the scraped data\n",
        "  df = pd.DataFrame(items_array, columns=['Name', 'Price', 'Condition'])\n",
        "  name_of_sheet = 'sheet' + str(sheet_number + 1)\n",
        "  #create file and transfers first dataframe\n",
        "  if sheet_number == 0:\n",
        "    with pd.ExcelWriter(file_name, engine='xlsxwriter') as writer:\n",
        "        df.to_excel(writer, index= False, sheet_name= name_of_sheet)\n",
        "  else:\n",
        "    # Write the DataFrame to an Excel file using xlsxwriter\n",
        "    with pd.ExcelWriter(file_name, mode= 'a', engine='openpyxl', if_sheet_exists= 'replace') as writer:\n",
        "      df.to_excel(writer, index= False, sheet_name= name_of_sheet)\n",
        "\n",
        "  # Read and display the Excel file\n",
        "  #df_read = pd.read_excel(file_name)\n",
        "  #print(df_read)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code to run:"
      ],
      "metadata": {
        "id": "wd4X4YtEpU1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#array containing all search pages to be scraped\n",
        "test_urls_arr = ['https://www.ebay.com.my/sch/i.html?_from=R40&_trksid=m570.l1313&_nkw=iPhone&_sacat=0&_odkw=iPhone+11&_osacat=0', 'https://www.ebay.com.my/sch/i.html?_nkw=casio+protrek&_sop=12']"
      ],
      "metadata": {
        "id": "C91pPe_wXqfW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = input('Enter file name: ')\n",
        "for i in range(0, len(test_urls_arr)):\n",
        "  num_pages = find_num_pages(test_urls_arr[i])\n",
        "  print(num_pages)\n",
        "  items_array = scrape_search_page(test_urls_arr[i], num_pages)\n",
        "  export_to_excel(items_array, i, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jsI0K4QTK-w",
        "outputId": "79f09c85-3bce-4bc3-ba7a-223e6444a0f6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter file name: scraped_data_result.xlsx\n",
            "367\n",
            "62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbnbd4Xxci4G"
      },
      "source": [
        "# Trying Selenium\n",
        "(doesnt work)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KXo8Q9gnkHuU",
        "outputId": "4325886e-d277-45e8-a5d5-98b3d6b8ca97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.22.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.25.1-py3-none-any.whl (467 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.7/467.7 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.6.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.22.0 trio-0.25.1 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install requests\n",
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "t8uN7GpIMjpc",
        "outputId": "d1d26521-3ae6-4500-c47b-6bef382bd2b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [973 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,125 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,664 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,591 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,994 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,409 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,258 kB]\n",
            "Fetched 13.4 MB in 2s (5,489 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'chromium-chromedriver' instead of 'chromium-driver'\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 9 newly installed, 0 to remove and 44 not upgraded.\n",
            "Need to get 28.5 MB of archives.\n",
            "After this operation, 118 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.3 [595 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.63+22.04 [25.9 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 28.5 MB in 2s (12.3 MB/s)\n",
            "E: Sub-process /usr/sbin/dpkg-preconfigure --apt || true received signal 2.\n",
            "E: Failure running script /usr/sbin/dpkg-preconfigure --apt || true\n",
            "debconf: apt-extracttemplates failed: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install chromium-driver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSOm93j-QuBa",
        "outputId": "b1e564d8-dd1a-4e65-ece2-b6a6d0606201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/usr/lib/chromium-browser/chromedriver': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls /usr/lib/chromium-browser/chromedriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "ayYSWqgyhHTo",
        "outputId": "443f3517-4e4f-4ca5-bd56-b0608b9e6490"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'expected' from 'selenium.webdriver.support' (/usr/local/lib/python3.10/dist-packages/selenium/webdriver/support/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-7145ea8ec320>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mui\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWebDriverWait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexpected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'expected' from 'selenium.webdriver.support' (/usr/local/lib/python3.10/dist-packages/selenium/webdriver/support/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# Set up Selenium with ChromeDriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--disable-gpu')\n",
        "options.add_argument(\"--window-size=1920, 1200\")\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Open the product page\n",
        "url = 'https://www.ebay.com.my/itm/355691390771?itmmeta=01J16M6TS1SWW3940J3WYD1SPA&hash=item52d0dbe733%3Ag%3Aa3sAAOSwR8xmOEzN&itmprp=enc%3AAQAJAAAA0DRrpJMK90GzYsnXI%2Bv%2BxUHgVsGiJ%2BkPfcZt2ZmSWz8y88vPiUT0UAlKDYT%2FsSckeQKxPmXBRZKQ4%2FIjrQN%2BFRBUrgA10hmCz1xRwSfe3jWO3SIEXJOZ4cWzrOBw1O4RMnK7wFYrScTc6YGbe3BaR%2BJG%2BfFv366CEPO6rvCCvjav7%2BPNshwmGRNlM8r3IvgYWco8E%2BGT0XImkQ6DPEE4Ct87%2FjLZezMeQAV5uJpBbbWMhdxCqP3Wtqq1zq8aMWxuLrTRXtoaqrCjynaPHpXb39M%3D%7Ctkp%3ABk9SR8qsm9SJZA&LH_BIN=1'\n",
        "driver = webdriver.Chrome(options=options)\n",
        "driver.get(url)\n",
        "\n",
        "WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR,'#root')))\n",
        "time.sleep(2)\n",
        "\n",
        "# Get the page source and parse it with BeautifulSoup\n",
        "page_source = driver.page_source\n",
        "soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "# Extract product information\n",
        "product_name = soup.find('h1', class_='attM6y')\n",
        "price = soup.find('div', class_='G27FPf')\n",
        "description = soup.find('div', class_='YPqix5')\n",
        "\n",
        "# Print the extracted information\n",
        "print(\"Product Name:\", product_name)\n",
        "print(\"Price:\", price)\n",
        "print(\"Description:\", description)\n",
        "\n",
        "# Close the WebDriver\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NLpT6sjiYaG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def scrape_ebay_search_results(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Your User Agent String'  # Set a proper user agent to avoid getting blocked\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract product data from the first page\n",
        "    product_data = parse_product_data(soup)\n",
        "\n",
        "    return product_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk3sHj2wlIXJ"
      },
      "outputs": [],
      "source": [
        "def scrape_all_pages(url, max_pages=3):\n",
        "    all_product_data = []\n",
        "\n",
        "    for page_num in range(1, max_pages + 1):\n",
        "        page_url = f\"{url}&_pgn={page_num}\"  # Modify the URL to include the page number\n",
        "        headers = {\n",
        "            'User-Agent': 'Your User Agent String'\n",
        "        }\n",
        "        response = requests.get(page_url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract product data from this page\n",
        "        page_product_data = parse_product_data(soup)\n",
        "\n",
        "        # Add the extracted data to the main list\n",
        "        all_product_data.extend(page_product_data)\n",
        "\n",
        "    return all_product_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DLmrcRjlRfv"
      },
      "outputs": [],
      "source": [
        "def parse_product_data(soup):\n",
        "    product_data = []\n",
        "    # Example: Assuming each product has a class \"product\"\n",
        "    products = soup.find_all('div', class_='product')\n",
        "\n",
        "    for product in products:\n",
        "        # Extract details like title, price, etc.\n",
        "        title = product.find('h2').text.strip()\n",
        "        price = product.find('span', class_='price').text.strip()\n",
        "        # Add more fields as necessary\n",
        "\n",
        "        # Store the data in a dictionary\n",
        "        product_info = {\n",
        "            'title': title,\n",
        "            'price': price,\n",
        "            # Add more fields as necessary\n",
        "        }\n",
        "\n",
        "        product_data.append(product_info)\n",
        "\n",
        "    return product_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGQR7QUKlTsG"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    ebay_url = \"https://www.ebay.com/sch/i.html?_nkw=laptop\"\n",
        "    all_product_data = scrape_all_pages(ebay_url)\n",
        "\n",
        "    # Print or process the collected product data\n",
        "    print(all_product_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNf6ZF568j_Y"
      },
      "source": [
        "# using encoding\n",
        "(doesnt work)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnMg07mu_9by"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "be65q84y8iwd",
        "outputId": "12a96ee6-3544-4ef5-9eb1-9ddef00eb908"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-f28c5a83083b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputlazada.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Search item on Lazada: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.lazada.com.my/tag/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "with open('outputlazada.txt', 'w') as w:\n",
        "  search = input('Search item on Lazada: ')\n",
        "  with requests.Session() as c:\n",
        "    url = 'https://www.lazada.com.my/tag/' + search\n",
        "    c.get(url)\n",
        "    search_data = dict(q = search)\n",
        "    page = c.post(url, data = search_data, headers = {\"Referer\" : \"www.lazada.com.my\"})\n",
        "    page = page.content\n",
        "    new_url = page.encode('utf-8')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zbnbd4Xxci4G",
        "pNf6ZF568j_Y"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}